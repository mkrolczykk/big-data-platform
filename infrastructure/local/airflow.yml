# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.
#
# WARNING: This configuration is for local development. Do not use it in a production deployment.
#
# This configuration supports basic configuration using environment variables or an .env file
# The following variables are supported:
#
# AIRFLOW_IMAGE_NAME         - Docker image name used to run Airflow.
#                              Default: -
# AIRFLOW_UID                - User ID in Airflow containers
#                              Default: 50000
# AIRFLOW_GID                - Group ID in Airflow containers
#                              Default: 50000
# _AIRFLOW_WWW_USER_USERNAME - Username for the administrator account.
#                              Default: airflow
# _AIRFLOW_WWW_USER_PASSWORD - Password for the administrator account.
#                              Default: airflow
#
---
version: '3.9'
x-airflow-common:
  &airflow-common
  user: root
  build:
    context: .
    dockerfile: airflow_dockerfile
  depends_on:
    redis:
      condition: service_healthy
    postgres_db:
      condition: service_healthy
#    vault:
#      condition: service_healthy
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DATABASE_USER}:${AIRFLOW_DATABASE_PASSWORD}@${POSTGRES_DB_HOST_NAME}/${AIRFLOW_DATABASE_NAME}
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${AIRFLOW_DATABASE_USER}:${AIRFLOW_DATABASE_PASSWORD}@${POSTGRES_DB_HOST_NAME}/${AIRFLOW_DATABASE_NAME}
    AIRFLOW__CELERY__BROKER_URL: redis://:@${REDIS_HOST_NAME}:${REDIS_HOST_INTERNAL_PORT}/0
    AIRFLOW__WEBSERVER__SECRET_KEY: '${AIRFLOW_WEBSERVER_SECRET_KEY}'
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__SECRETS__BACKEND: airflow.providers.hashicorp.secrets.vault.VaultBackend
    AIRFLOW__SECRETS__BACKEND_KWARGS: '{"url":"http://${VAULT_HOST_NAME}:${VAULT_HOST_INTERNAL_PORT}","token":"${VAULT_DEV_ROOT_TOKEN_ID}","mount_point":"${VAULT_MOUNT_POINT}","variables_path":"${VAULT_VARIABLES_PATH}","connections_path":"${VAULT_CONNECTIONS_PATH}"}'
    AIRFLOW__CORE__STORE_DAG_CODE: 'true'
    AIRFLOW__CORE__STORE_SERIALIZED_DAGS: 'true'
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 60
    AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: 15
    AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL: 30
    AIRFLOW__WEBSERVER__SHOW_TRIGGER_FORM_IF_NO_PARAMS: 'true'
    AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL: 30
    AIRFLOW__CORE__MIN_SERIALIZED_DAG_FETCH_INTERVAL: 10
    AIRFLOW__SMART_SENSOR__USE_SMART_SENSOR: 'true'
    AIRFLOW__SMART_SENSOR__SHARD_CODE_UPPER_LIMIT: 10000
    AIRFLOW__SMART_SENSOR__SHARDS: 3
    AIRFLOW__SMART_SENSOR__SENSORS_ENABLED: NamedHivePartitionSensor, MetastorePartitionSensor, FileExtendedSensor
  env_file:
    - ./.env
    - ./../../spark/.env
  volumes:
    - ./containers-data/airflow/logs:/opt/airflow/logs
    - /var/run/docker.sock:/var/run/docker.sock
    - ./../../airflow/dags:/opt/airflow/dags
    - ./../../airflow/plugins:/opt/airflow/plugins
    - ./../../airflow/projects:/opt/airflow/projects
    - ./../../airflow/scripts:/opt/airflow/scripts
    - ./../../airflow/tests:/opt/airflow/tests
    - ./../../airflow/requirements.txt:/opt/airflow/requirements.txt
    - ./../../spark/jobs:/opt/airflow/spark/jobs
    - ./../../spark/configs:/opt/airflow/spark/configs
    - ./../../spark/dependencies:/opt/airflow/spark/dependencies
    - ./../../spark/tests:/opt/airflow/spark/tests
    - ./../../spark/packages.zip:/opt/airflow/spark/packages.zip
    - ./../../spark/build_dependencies.sh:/opt/airflow/spark/build_dependencies.sh
    - ./../../spark/Pipfile.lock:/opt/airflow/spark/Pipfile.lock
  networks:
    - ${GLOBAL_NETWORK:-services}

services:
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    command: version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}

  airflow_webserver:
    <<: *airflow-common
    container_name: airflow_webserver
    command: webserver
    ports:
      - "${AIRFLOW_WEBSERVER_HOST_PORT}:${AIRFLOW_WEBSERVER_HOST_INTERNAL_PORT}"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:${AIRFLOW_WEBSERVER_HOST_PORT}/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    volumes:
      - ./containers-data/airflow/webserver:/var/lib/postgresql/data/:z

  airflow_scheduler-1:
    <<: *airflow-common
    container_name: airflow_scheduler-1
    command: scheduler
    restart: always

  airflow_worker-1:
    <<: *airflow-common
    container_name: airflow_worker-1
    command: celery worker
    restart: always

  airflow_worker-2:
    <<: *airflow-common
    container_name: airflow_worker-2
    command: celery worker -q jobs_queue
    restart: always

#  airflow_worker-3:
#    <<: *airflow-common
#    container_name: airflow_worker-3
#    command: celery worker -q jobs_queue
#    restart: always

  flower:
    <<: *airflow-common
    container_name: flower
    command: celery flower
    ports:
      - "${FLOWER_HOST_PORT}:${FLOWER_HOST_INTERNAL_PORT}"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:${FLOWER_HOST_PORT}/"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always